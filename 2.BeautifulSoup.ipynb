{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter2 BeautifulSoup\n",
    "## 1. BeautifulSoup란?\n",
    "BeautifulSoup는 HTML 또는 XML 문서를 구문분석하기 위한 라이브러리 입니다.  \n",
    "Chapter1에서는 requests 모듈을 사용하여 HTTP request를 날리는 방법을 배웠습니다.  \n",
    "\n",
    "response의 형식에는 json, html, xml이 주를 이룬다고 했는데 json의 경우에는 json 모듈을 사용하여  \n",
    "python의 dict 형태로 파싱이 가능하지만 html과 xml은 json과는 다소 차이가 있기에 해당 챕터에서는 다루지 않았습니다.  \n",
    "\n",
    "이번 장에서는 html과 xml을 파싱할 수 있는 라이브러리인 BeautifulSoup에 대해 다뤄보도록 하겠습니다.  \n",
    "\n",
    "## 2. BeautifulSoup 시작하기\n",
    "1장에서와 마찬가지로 beautifulsoup가 설치되어 있는지 확인하고,  \n",
    "만약 설치되지 않았다면 설치를 하도록 하겠습니다.  \n",
    "\n",
    "pip 환경에서는  \n",
    "- 설치 확인  \n",
    "```bash\n",
    "pip list | findstr beautifulsoup4\n",
    "```\n",
    "- 설치  \n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "conda 환경에서는\n",
    "- 설치 확인  \n",
    "```bash\n",
    "conda list | findstr beautifulsoup4\n",
    "```\n",
    "- 설치  \n",
    "```bash\n",
    "conda install beautifulsoup4\n",
    "```\n",
    "\n",
    "설치가 완료되었으면 다음 셀을 실행하여 import 하고 requests 모듈을 이용하여 네이버의 메인 페이지를 불러온 후  \n",
    "response.text와 'html.parser'를 BeautifulSoup 객체의 생성자에 인자로 넣어줍니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "response=requests.get('https://www.naver.com')\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 챕터에서 response.text와 다른 점은 BeautifulSoup를 사용하면 후술할 CSS selector로 HTML(XML) 구문 분석이 가능해진다는 점입니다.  \n",
    "HTML 문서는 크게 다음과 같은 형식을 띠고 있습니다.  \n",
    "```html\n",
    "<!DOCTTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <!--title, meta, script 또는 link 태그-->\n",
    "    </head>\n",
    "    <body>\n",
    "        <!--문서의 본문-->\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "다음 셀을 실행하여 네이버의 body 부분만 따로 출력해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.select('body')\n",
    "print(body[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSS Selector\n",
    "위 셀의 soup.select()는 CSS selector를 인자로 받아 CSS Selector에 해당하는 Tag들을 리스트로 구성하여 반환해줍니다.  \n",
    "그렇다면 CSS가 우선 무엇인지 알아볼 필요가 있습니다.  \n",
    "\n",
    "흔히 우리가 접하는 웹페이지는 3가지의 언어로 이루어져 있습니다.  \n",
    "1. HTML\n",
    "1. CSS\n",
    "1. Javascript\n",
    "\n",
    "HTML은 내용물을 구성하고(글, 이미지, 표 등), CSS는 위 내용물을 꾸미는 역할을 하며, javascript는 웹페이지에 변동을 주거나 서버와의 비동기 통신(Ajax)등을 수행합니다.  \n",
    "\n",
    "CSS Selector는 어떤 html 태그에 스타일을 적용할지 선택하기 위해 사용됩니다.  \n",
    "BeautifulSoup를 사용하게 되면 이러한 CSS Selector를 웹 크롤링에도 적용할 수 있습니다.  \n",
    "\n",
    "CSS Selector만 잘 구성한다면 html 문서에 있는 모든 태그에 접근이 가능하고 값을 추출할 수 있는것입니다.  \n",
    "그렇다면 CSS Selector가 무엇인지 알아보도록 하겠습니다.  \n",
    "\n",
    "html 문서의 p 태그는 다음과 같이 이루어져 있습니다.  \n",
    "```html\n",
    "<p id=\"p_id\" class=\"p_class\" some_attr=\"some_attr_value\">Some Text</p>\n",
    "```\n",
    "p 태그는 id가 p_id이고, class가 p_class 입니다.  \n",
    "\n",
    "\n",
    "해당 태그에 접근하는 방법은 세가지 입니다.  \n",
    "1. 태그 그 자체로 접근하는 방법. ex) p\n",
    "1. 태그의 id로 접근하는 방법. ex) #p_id\n",
    "1. 태그의 class로 접근하는 방법. ex) .p_class\n",
    "\n",
    "해당 선택자는 서로 같이 쓸수도 있습니다.  \n",
    "예를들어 p.p_class나 p#p_id 또는 p.p_class1.p_class2 이런식으로 작성이 가능합니다.  \n",
    "\n",
    "하지만 네이버와 같은 큰 규모의 웹 페이지에서는 CSS Selector만으로 찾고자 하는 태그를 찾아내기는 힘들며,  \n",
    "다른 selector간의 관계로 표현해야 하는 경우가 빈번히 발생합니다.  \n",
    "이에 사용되는 것이 CSS Combinator입니다.  \n",
    "CSS Combinator에는 4가지가 있습니다.  \n",
    "1. Descendant Selector ex) div p\n",
    "1. Child Selector ex) div > p\n",
    "1. Adjacent Sibling Selector ex) div + p\n",
    "1. General Sibling Selector ex) div ~ p\n",
    "\n",
    "첫번째 자손 선택자는 첫번째 선택자 아래에 위치하는 모든 두번째 선택자를 선택합니다.  \n",
    "두번째 자식 선택자는 첫번째 선택자 바로 아래에 위치하는 모든 두번째 선택자를 선택합니다.  \n",
    "세번째 인접 형제 선택자는 첫번째 선택자 바로 옆에 위치하는 모든 두번째 선택자를 선택합니다.  \n",
    "네번째 일반 형재 선택자는 첫번째 선택자 옆에 위치하는 모든 두번째 선택자를 선택합니다. \n",
    "\n",
    "이런 선택자들을 어떻게 찾아야 할까요?  \n",
    "\n",
    "물론 html 문서를 처음부터 읽어나가면서  \n",
    "html > body > div#u_skip  \n",
    "이런식으로 찾아나갈 수 있지만 크롬의 개발자도구(F12)를 이용한다면 더욱 편하게 CSS Selector를 찾을 수 있습니다.  \n",
    "\n",
    "Chapter1에서는 requests 모듈을 사용하여 검색어를 입력하면 검색결과 html 문서를 호출하는 실습을 작성했습니다.  \n",
    "이번에는 검색결과에서 뉴스 기사의 제목들을 뽑아보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 문자열을 입력해 주세요: 하트시그널\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "keyword=input('검색할 문자열을 입력해 주세요: ')\n",
    "response=requests.get('https://search.naver.com/search.naver',params={'where':'news','sm':'top_hty','fbm':'0','ie':'utf8','query':keyword})\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 셀을 실행하면 검색결과의 웹 페이지가 soup 객체에 저장이 됩니다.  \n",
    "\n",
    "우리는 뉴스를 확인하고 싶은것이기 때문에  \n",
    "크롬을 실행해 검색결과 창에 접속합니다.  \n",
    "그리고 F12를 누르고 elements 탭의 해당 버튼을 클릭합니다.  \n",
    "![2-1](./image/2-1.png)\n",
    "\n",
    "해당 버튼을 클릭하고 웹 페이지에 우리가 알고 싶었던 부분인 뉴스 기사의 제목을 클릭하면  \n",
    "오른쪽의 Elements 탭에 우리가 클릭한 부분이 실제 html 문서에서 어디에 위치하고 있는 지 강조가 됩니다.  \n",
    "\n",
    "해당 태그를 우클릭하게 되면 다음과 같이 selector를 선택할 수 있게 됩니다.  \n",
    "![2-2](./image/2-2.png)\n",
    "\n",
    "복사한 css selector는 다음과 같습니다.  \n",
    "#sp_nws1 > dl > dt > a  \n",
    "\n",
    "다음 셀을 실행하여 selector가 제대로 복사가 되었는지 확인해 봅시다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"_sp_each_title\" href=\"https://www.ytn.co.kr/_sn/0117_202005141122528335\" onclick=\"return goOtherCR(this, 'a=nws*a.tit&amp;r=1&amp;i=880000AF_000000000000000001439924&amp;g=052.0001439924&amp;u='+urlencode(this.href));\" target=\"_blank\" title=\"[Y초점] '하트시그널3' 학폭논란 속 첫 등장하는 천안나\">[Y초점] '<strong class=\"hl\">하트시그널</strong>3' 학폭논란 속 첫 등장하는 천안나</a>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles=soup.select('#sp_nws1 > dl > dt > a')\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "태그는 정상적으로 출력이 됐지만 우리가 가져오고자 하는 대상은 해당 웹페이지의 여러개의 뉴스입니다.  \n",
    "따라서 selector를 다소 수정할 필요가 있어 보입니다.  \n",
    "\n",
    "사용한 selector의 가장 첫번째 부분은 #sp_nws1입니다.  \n",
    "의미상 첫번째 기사를 의미하는것 같습니다.  \n",
    "이 부분을 li로 바꾸고 셀을 실행하도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=soup.select('li > dl > dt > a')\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 여러 기사의 제목에 해당하는 태그를 읽어온것을 알 수 있습니다.  \n",
    "\n",
    "하지만 우리가 원한것은 태그가 아닌 기사의 제목입니다.  \n",
    "따라서 각 태그의 text를 출력하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Y초점] '하트시그널3' 학폭논란 속 첫 등장하는 천안나\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성공적으로 기사의 제목이 출력되었습니다.  \n",
    "a 태그를 살펴보니 text 말고도 해당 태그의 title 속성에도 우리가 찾고자 했던 기사의 제목이 해당된다는 것을 알 수 있습니다.  \n",
    "\n",
    "이번에는 text가 아닌 태그의 title 속성에 get() 함수를 이용하여 접근해보도록 하겠습니다.  \n",
    "다음 셀을 실행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    print(title.get(\"title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assignment\n",
    "\n",
    "아래 셀에  \n",
    "멜론 차트(https://www.melon.com/chart/index.htm) 에서  \n",
    "음원 순위를 1위부터 50위까지(가능하다면 100위까지도) 다음 형식으로 출력해 주는 소스코드를 작성해 주세요.  \n",
    "```\n",
    "1위: 아이유 - 에잇(Prod.&Feat. SUGA of BTS)\n",
    "2위: 볼빨간사춘기 - 나비와 고양이 (feat.백현 (BAEKHYUN))\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Write your source code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
